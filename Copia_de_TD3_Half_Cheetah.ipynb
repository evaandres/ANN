{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de TD3_Half_Cheetah.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y2nGdtlKVydr",
        "Jb7TTaHxWbQD",
        "HRDDce8FXef7",
        "ka-ZRtQvjBex",
        "gGuKmH_ijf7U",
        "Hjwf2HCol3XP",
        "kop-C96Aml8O",
        "qEAzOd47mv1Z",
        "5YdPG4HXnNsh",
        "HWEgDAQxnbem",
        "ZI60VN2Unklh",
        "QYOpCyiDnw7s",
        "xm-4b3p6rglE",
        "31n5eb03p-Fm",
        "q9gsjvtPqLgT",
        "wi6e2-_pu05e"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evaandres/ANN/blob/master/Copia_de_TD3_Half_Cheetah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Gradiente de política determinista profunda (TD3) de doble retardo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Instalación de los paquetes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Paso 1: Inicializamos la memoria de la repetición de experiencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage)== self.max_size: ####OJO, en el vídeo ponía MAX_STORAGE!!!\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size = batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy = False))\n",
        "      batch_next_states.append(np.array(next_state, copy = False))\n",
        "      batch_actions.append(np.array(action, copy = False))\n",
        "      batch_rewards.append(np.array(reward, copy = False))\n",
        "      batch_dones.append(np.array(done, copy = False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Paso 2: Construimos una red neuronal para el **actor del modelo** y una red neuronal para el **actor del objetivo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7"
      },
      "source": [
        "\n",
        "## Paso 3: Construimos dos redes neuronales para los dos **críticos del modelo** y dos redes neuronales para los dos **críticos del objetivo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Definimos el primero de los Críticos como red neuronal profunda\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Definimos el segundo de los Críticos como red neuronal profunda\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Propagación hacia adelante del primero de los Críticos\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Propagación hacia adelante del segundo de los Críticos\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "  \n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Pasos 4 a 15: Proceso de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "source": [
        "# Selección del dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construir todo el proceso de entrenamiento en una clase\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clipping=0.5, policy_freq=2):\n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Paso 4: Tomamos una muestra de transiciones (s, s’, a, r) de la memoria.\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Paso 5: A partir del estado siguiente s', el Actor del Target ejecuta la siguiente acción a'.\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Paso 6: Añadimos ruido gaussiano a la siguiente acción a' y lo cortamos para tenerlo en el rango de valores aceptado por el entorno.\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device) \n",
        "      noise = noise.clamp(-noise_clipping, noise_clipping)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Paso 7: Los dos Críticos del Target toman un par (s’, a’) como entrada y devuelven dos Q-values Qt1(s’,a’) y Qt2(s’,a’) como salida.\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Paso 8: Nos quedamos con el mínimo de los dos Q-values: min(Qt1, Qt2). Representa el valor aproximado del estado siguiente.\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Paso 9: Obtenemos el target final de los dos Crítico del Modelo, que es: Qt = r + γ * min(Qt1, Qt2), donde γ es el factor de descuento.\n",
        "      target_Q = reward + ((1-done) * discount * target_Q).detach()\n",
        "\n",
        "      # Paso 10: Los dos Críticos del Modelo toman un par (s, a) como entrada y devuelven dos Q-values Q1(s,a) y Q2(s,a) como salida.\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Paso 11: Calculamos la pérdida procedente de los Crítico del Modelo: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Paso 12: Propagamos hacia atrás la pérdida del crítico y actualizamos los parámetros de los dos Crítico del Modelo con un SGD.\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Paso 13: Cada dos iteraciones, actualizamos nuestro modelo de Actor ejecutando el gradiente ascendente en la salida del primer modelo crítico.\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()##OJO ME DEJÉ EL LOSS\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Paso 14: Todavía cada dos iteraciones, actualizamos los pesos del Actor del Target usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "        # Paso 15: Todavía cada dos iteraciones, actualizamos los pesos del target del Crítico usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "  # Método para guardar el modelo entrenado\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n",
        "\n",
        "  # Método para cargar el modelo entrenado\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load(\"%s/%s_actor.pth\" % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load(\"%s/%s_critic.pth\" % (directory, filename)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## Hacemos una función que evalúa la política calculando su recompensa promedio durante 10 episodios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"-------------------------------------------------\")\n",
        "  print (\"Recompensa promedio en el paso de Evaluación: %f\" % (avg_reward))\n",
        "  print (\"-------------------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## Configuramos los parámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "source": [
        "env_name = \"HalfCheetahBulletEnv-v0\" # Nombre del entorno (puedes indicar cualquier entorno continuo que quieras probar aquí)\n",
        "seed = 0 # Valor de la semilla aleatoria\n",
        "start_timesteps = 1e4 # Número de of iteraciones/timesteps durante las cuales el modelo elige una acción al azar, y después de las cuales comienza a usar la red de políticas\n",
        "eval_freq = 5e3 # Con qué frecuencia se realiza el paso de evaluación (después de cuántos pasos timesteps)\n",
        "max_timesteps = 5e5 # Número total de iteraciones/timesteps\n",
        "save_models = True # Check Boolean para saber si guardar o no el modelo pre-entrenado\n",
        "expl_noise = 0.1 # Ruido de exploración: desviación estándar del ruido de exploración gaussiano\n",
        "batch_size = 100 # Tamaño del bloque\n",
        "discount = 0.99 # Factor de descuento gamma, utilizado en el cáclulo de la recompensa de descuento total\n",
        "tau = 0.005 # Ratio de actualización de la red de objetivos\n",
        "policy_noise = 0.2 # Desviación estándar del ruido gaussiano añadido a las acciones para fines de exploración\n",
        "noise_clip = 0.5 # Valor máximo de ruido gaussiano añadido a las acciones (política)\n",
        "policy_freq = 2 # Número de iteraciones a esperar antes de actualizar la red de políticas (actor modelo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## Creamos un nombre de archivo para los dos modelos guardados: los modelos Actor y Crítico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c8450387-8b30-438e-c27f-78a88e6c63ff"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Configuración: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Configuración: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## Creamos una carpeta dentro de la cual se guardarán los modelos entrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## Creamos un entorno de `PyBullet`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "84172738-f7ac-4f4a-ee18-8b74917751fb"
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## Establecemos las semillas y obtenemos la información necesaria sobre los estados y las acciones en el entorno elegido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj"
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## Creamos la red neronal de la política (el actor del modelo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg"
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## Creamos la memoria de la repetición de experiencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## Definimos una lista donde se guardaran los resultados de evaluación de los 10 episodios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "73e7ba44-93c1-4374-d37d-762940fc3ee0"
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 9.804960\n",
            "-------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## Creamos un nuevo directorio de carpetas en el que se mostrarán los resultados finales (videos del agente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## Inicializamos las variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb207338-7818-41f5-e568-88ba8440c812"
      },
      "source": [
        "# Iniciamos el bucle principal con un total de 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # Si el episodio ha terminado\n",
        "  if done:\n",
        "\n",
        "    # Si no estamos en la primera de las iteraciones, arrancamos el proceso de entrenar el modelo\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # Evaluamos el episodio y guardamos la política si han pasado las iteraciones necesarias\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # Cuando el entrenamiento de un episodio finaliza, reseteamos el entorno\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Configuramos el valor de done a False\n",
        "    done = False\n",
        "    \n",
        "    # Configuramos la recompensa y el timestep del episodio a cero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Antes de los 10000 timesteps, ejectuamos acciones aleatorias\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # Después de los 10000 timesteps, cambiamos al modelo\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # Si el valor de explore_noise no es 0, añadimos ruido a la acción y lo recortamos en el rango adecuado\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # El agente ejecuta una acción en el entorno y alcanza el siguiente estado y una recompensa\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # Comprobamos si el episodio ha terminado\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # Incrementamos la recompensa total\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # Almacenamos la nueva transición en la memoria de repetición de experiencias (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # Actualizamos el estado, el timestep del número de episodio, el total de timesteps y el número de pasos desde la última evaluación de la política\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# Añadimos la última actualización de la política a la lista de evaluaciones previa y guardamos nuestro modelo\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 999 Episode Num: 1 Reward: 524.6170436692805\n",
            "Total Timesteps: 1127 Episode Num: 2 Reward: 63.55716677702583\n",
            "Total Timesteps: 2127 Episode Num: 3 Reward: 490.4700079582326\n",
            "Total Timesteps: 3127 Episode Num: 4 Reward: 472.442099396563\n",
            "Total Timesteps: 4127 Episode Num: 5 Reward: 409.3833871953717\n",
            "Total Timesteps: 4882 Episode Num: 6 Reward: 350.2987062980421\n",
            "Total Timesteps: 5882 Episode Num: 7 Reward: 500.42375278671125\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 85.196727\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 6882 Episode Num: 8 Reward: 484.8578477454199\n",
            "Total Timesteps: 7882 Episode Num: 9 Reward: 502.8161869620753\n",
            "Total Timesteps: 7967 Episode Num: 10 Reward: 36.51102120020369\n",
            "Total Timesteps: 8967 Episode Num: 11 Reward: 524.5595556844827\n",
            "Total Timesteps: 9182 Episode Num: 12 Reward: 94.96814609495775\n",
            "Total Timesteps: 9442 Episode Num: 13 Reward: 125.61739133636148\n",
            "Total Timesteps: 10442 Episode Num: 14 Reward: 532.9900396966937\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 77.142357\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 11442 Episode Num: 15 Reward: 86.03368752666786\n",
            "Total Timesteps: 12442 Episode Num: 16 Reward: 193.25384393153047\n",
            "Total Timesteps: 13442 Episode Num: 17 Reward: 107.29062661131209\n",
            "Total Timesteps: 14442 Episode Num: 18 Reward: 103.06614543583561\n",
            "Total Timesteps: 15442 Episode Num: 19 Reward: 83.0047744706729\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 223.246750\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 16442 Episode Num: 20 Reward: 233.8891663305038\n",
            "Total Timesteps: 17442 Episode Num: 21 Reward: 510.4705626986918\n",
            "Total Timesteps: 18442 Episode Num: 22 Reward: 410.6419386624537\n",
            "Total Timesteps: 19442 Episode Num: 23 Reward: 136.40051835837727\n",
            "Total Timesteps: 20442 Episode Num: 24 Reward: 321.80779150656724\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 347.651608\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 21442 Episode Num: 25 Reward: 311.0394027791997\n",
            "Total Timesteps: 22442 Episode Num: 26 Reward: 537.7927704147434\n",
            "Total Timesteps: 23442 Episode Num: 27 Reward: 502.2214222002943\n",
            "Total Timesteps: 24442 Episode Num: 28 Reward: 562.2233607069426\n",
            "Total Timesteps: 25442 Episode Num: 29 Reward: 466.482802810801\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 353.071239\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 26442 Episode Num: 30 Reward: 368.37163121761427\n",
            "Total Timesteps: 27442 Episode Num: 31 Reward: 313.34606643969414\n",
            "Total Timesteps: 28442 Episode Num: 32 Reward: 335.35511406076364\n",
            "Total Timesteps: 29442 Episode Num: 33 Reward: 199.76211341927745\n",
            "Total Timesteps: 30442 Episode Num: 34 Reward: 366.2540302936835\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 258.641603\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 31442 Episode Num: 35 Reward: 150.5938301761841\n",
            "Total Timesteps: 32442 Episode Num: 36 Reward: 410.79484350992084\n",
            "Total Timesteps: 33442 Episode Num: 37 Reward: 332.11950237942693\n",
            "Total Timesteps: 34442 Episode Num: 38 Reward: 222.3241465918668\n",
            "Total Timesteps: 35442 Episode Num: 39 Reward: 305.67572335227817\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 187.707616\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 36442 Episode Num: 40 Reward: 173.78268483845926\n",
            "Total Timesteps: 37442 Episode Num: 41 Reward: 321.4880378039608\n",
            "Total Timesteps: 38442 Episode Num: 42 Reward: 555.4762313198099\n",
            "Total Timesteps: 38781 Episode Num: 43 Reward: 152.6393884488495\n",
            "Total Timesteps: 39781 Episode Num: 44 Reward: 538.8825560444246\n",
            "Total Timesteps: 40781 Episode Num: 45 Reward: 423.9479910906343\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 462.899838\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 41781 Episode Num: 46 Reward: 453.2514859858107\n",
            "Total Timesteps: 42781 Episode Num: 47 Reward: 355.49142043996414\n",
            "Total Timesteps: 43781 Episode Num: 48 Reward: 463.9254002337074\n",
            "Total Timesteps: 44781 Episode Num: 49 Reward: 149.02501944085677\n",
            "Total Timesteps: 45781 Episode Num: 50 Reward: 321.1916812904941\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 324.523661\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 46781 Episode Num: 51 Reward: 469.3619991378323\n",
            "Total Timesteps: 47781 Episode Num: 52 Reward: 303.68289709785546\n",
            "Total Timesteps: 48781 Episode Num: 53 Reward: 173.96085122127832\n",
            "Total Timesteps: 49781 Episode Num: 54 Reward: 331.3267002229737\n",
            "Total Timesteps: 50781 Episode Num: 55 Reward: 338.8282628538782\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 209.973424\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 51781 Episode Num: 56 Reward: 211.8238234623104\n",
            "Total Timesteps: 52781 Episode Num: 57 Reward: 178.9207892925617\n",
            "Total Timesteps: 53781 Episode Num: 58 Reward: 200.86019964982637\n",
            "Total Timesteps: 54781 Episode Num: 59 Reward: 327.14268004454846\n",
            "Total Timesteps: 55781 Episode Num: 60 Reward: 414.8513814422839\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 47.244578\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 55802 Episode Num: 61 Reward: 4.742465266196577\n",
            "Total Timesteps: 56802 Episode Num: 62 Reward: 523.49921996501\n",
            "Total Timesteps: 57802 Episode Num: 63 Reward: 430.80245131718425\n",
            "Total Timesteps: 58802 Episode Num: 64 Reward: 541.0908722730683\n",
            "Total Timesteps: 59802 Episode Num: 65 Reward: 265.3325849485923\n",
            "Total Timesteps: 60802 Episode Num: 66 Reward: 347.6083062759481\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 272.022485\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 61802 Episode Num: 67 Reward: 528.9432839074155\n",
            "Total Timesteps: 62802 Episode Num: 68 Reward: 220.09098453796162\n",
            "Total Timesteps: 63802 Episode Num: 69 Reward: 191.09665839934178\n",
            "Total Timesteps: 64802 Episode Num: 70 Reward: 397.7736583031064\n",
            "Total Timesteps: 65802 Episode Num: 71 Reward: 321.91708025556056\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 308.627138\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 66802 Episode Num: 72 Reward: 293.46455872140757\n",
            "Total Timesteps: 67802 Episode Num: 73 Reward: 242.82406237095867\n",
            "Total Timesteps: 68802 Episode Num: 74 Reward: 261.90953577827423\n",
            "Total Timesteps: 69802 Episode Num: 75 Reward: 319.3543824699257\n",
            "Total Timesteps: 70802 Episode Num: 76 Reward: 302.7274023699932\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 357.028739\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 71802 Episode Num: 77 Reward: 428.39800888229763\n",
            "Total Timesteps: 72802 Episode Num: 78 Reward: 314.204993554579\n",
            "Total Timesteps: 73802 Episode Num: 79 Reward: 428.7907874471497\n",
            "Total Timesteps: 74802 Episode Num: 80 Reward: 217.33296628367302\n",
            "Total Timesteps: 75802 Episode Num: 81 Reward: 309.6647340822539\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 452.420689\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 76802 Episode Num: 82 Reward: 406.5310436102397\n",
            "Total Timesteps: 77802 Episode Num: 83 Reward: 333.416466970662\n",
            "Total Timesteps: 78802 Episode Num: 84 Reward: 245.68560120496088\n",
            "Total Timesteps: 79802 Episode Num: 85 Reward: 443.9872139734735\n",
            "Total Timesteps: 80802 Episode Num: 86 Reward: 283.6857603987843\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 298.601710\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 81802 Episode Num: 87 Reward: 290.84202424703045\n",
            "Total Timesteps: 82802 Episode Num: 88 Reward: 545.1935720365274\n",
            "Total Timesteps: 83802 Episode Num: 89 Reward: 310.9619518901932\n",
            "Total Timesteps: 84802 Episode Num: 90 Reward: 475.0368628885491\n",
            "Total Timesteps: 85802 Episode Num: 91 Reward: 278.59506053994085\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 383.778393\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 86802 Episode Num: 92 Reward: 374.29060268245274\n",
            "Total Timesteps: 87802 Episode Num: 93 Reward: 369.2698494452138\n",
            "Total Timesteps: 88802 Episode Num: 94 Reward: 214.17412951030434\n",
            "Total Timesteps: 89802 Episode Num: 95 Reward: 349.5991081424461\n",
            "Total Timesteps: 90802 Episode Num: 96 Reward: 151.6141432185964\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 216.177320\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 91802 Episode Num: 97 Reward: 132.52827224241707\n",
            "Total Timesteps: 92066 Episode Num: 98 Reward: 77.15750713703822\n",
            "Total Timesteps: 92501 Episode Num: 99 Reward: 106.04043972867295\n",
            "Total Timesteps: 93501 Episode Num: 100 Reward: 490.63849643766156\n",
            "Total Timesteps: 94501 Episode Num: 101 Reward: 364.8393750578134\n",
            "Total Timesteps: 95501 Episode Num: 102 Reward: 330.8685018997331\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 453.877997\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 96501 Episode Num: 103 Reward: 469.4419121848119\n",
            "Total Timesteps: 97501 Episode Num: 104 Reward: 546.3839647320542\n",
            "Total Timesteps: 98501 Episode Num: 105 Reward: 539.8943091543754\n",
            "Total Timesteps: 99501 Episode Num: 106 Reward: 473.46867034174215\n",
            "Total Timesteps: 100501 Episode Num: 107 Reward: 418.35917971394684\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 545.656689\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 101501 Episode Num: 108 Reward: 524.7207450994626\n",
            "Total Timesteps: 102501 Episode Num: 109 Reward: 503.6228410070396\n",
            "Total Timesteps: 103501 Episode Num: 110 Reward: 446.501219893845\n",
            "Total Timesteps: 104501 Episode Num: 111 Reward: 539.8348468397244\n",
            "Total Timesteps: 104521 Episode Num: 112 Reward: 6.99761089849843\n",
            "Total Timesteps: 104541 Episode Num: 113 Reward: 6.159979118889249\n",
            "Total Timesteps: 104561 Episode Num: 114 Reward: 6.455687901876539\n",
            "Total Timesteps: 104581 Episode Num: 115 Reward: 6.274186452892883\n",
            "Total Timesteps: 104601 Episode Num: 116 Reward: 6.48581495738748\n",
            "Total Timesteps: 104621 Episode Num: 117 Reward: 6.077138956672071\n",
            "Total Timesteps: 104641 Episode Num: 118 Reward: 7.0558695325386935\n",
            "Total Timesteps: 104820 Episode Num: 119 Reward: 77.43759091613522\n",
            "Total Timesteps: 104840 Episode Num: 120 Reward: 4.244884403930481\n",
            "Total Timesteps: 104860 Episode Num: 121 Reward: 5.989036366582609\n",
            "Total Timesteps: 104880 Episode Num: 122 Reward: 4.5494689346218955\n",
            "Total Timesteps: 104900 Episode Num: 123 Reward: 6.203908544458754\n",
            "Total Timesteps: 104920 Episode Num: 124 Reward: 6.39832012691933\n",
            "Total Timesteps: 105920 Episode Num: 125 Reward: 563.7588439902643\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 707.530254\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 106920 Episode Num: 126 Reward: 628.8705076181499\n",
            "Total Timesteps: 107920 Episode Num: 127 Reward: 300.88711942565845\n",
            "Total Timesteps: 108920 Episode Num: 128 Reward: 157.40653239583708\n",
            "Total Timesteps: 109920 Episode Num: 129 Reward: 397.9365405723605\n",
            "Total Timesteps: 110920 Episode Num: 130 Reward: 622.4092769567917\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 448.008666\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 111920 Episode Num: 131 Reward: 536.9268824634203\n",
            "Total Timesteps: 112920 Episode Num: 132 Reward: 429.0337877974406\n",
            "Total Timesteps: 113920 Episode Num: 133 Reward: 468.7630762064135\n",
            "Total Timesteps: 114920 Episode Num: 134 Reward: 503.2756946645957\n",
            "Total Timesteps: 115920 Episode Num: 135 Reward: 350.84558060529673\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 430.307626\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 116245 Episode Num: 136 Reward: 135.9044559218923\n",
            "Total Timesteps: 117245 Episode Num: 137 Reward: 354.2433663016785\n",
            "Total Timesteps: 118245 Episode Num: 138 Reward: 234.9080362881276\n",
            "Total Timesteps: 119245 Episode Num: 139 Reward: 559.4876063217831\n",
            "Total Timesteps: 120245 Episode Num: 140 Reward: 206.8012711398436\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 285.204844\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 120769 Episode Num: 141 Reward: 243.62663326587702\n",
            "Total Timesteps: 120893 Episode Num: 142 Reward: 32.355641688572256\n",
            "Total Timesteps: 121083 Episode Num: 143 Reward: 59.7637902785591\n",
            "Total Timesteps: 122083 Episode Num: 144 Reward: 415.9969082488822\n",
            "Total Timesteps: 123083 Episode Num: 145 Reward: 620.9758211207111\n",
            "Total Timesteps: 124083 Episode Num: 146 Reward: 411.32154592604803\n",
            "Total Timesteps: 125083 Episode Num: 147 Reward: 334.8182898659884\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 358.815745\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 126083 Episode Num: 148 Reward: 381.4464444374202\n",
            "Total Timesteps: 127083 Episode Num: 149 Reward: 256.84567437673604\n",
            "Total Timesteps: 128083 Episode Num: 150 Reward: 506.7070004044887\n",
            "Total Timesteps: 129083 Episode Num: 151 Reward: 275.8342964825858\n",
            "Total Timesteps: 130083 Episode Num: 152 Reward: 450.8344631632277\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 329.503732\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 131083 Episode Num: 153 Reward: 626.1585760669947\n",
            "Total Timesteps: 132083 Episode Num: 154 Reward: 492.6320991508485\n",
            "Total Timesteps: 133083 Episode Num: 155 Reward: 446.31026250585273\n",
            "Total Timesteps: 134083 Episode Num: 156 Reward: 366.37521223844044\n",
            "Total Timesteps: 135083 Episode Num: 157 Reward: 318.860421356148\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 488.398516\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 136083 Episode Num: 158 Reward: 317.60572595380023\n",
            "Total Timesteps: 137083 Episode Num: 159 Reward: 469.29763088542694\n",
            "Total Timesteps: 138083 Episode Num: 160 Reward: 568.8368036455906\n",
            "Total Timesteps: 139083 Episode Num: 161 Reward: 597.6023021246331\n",
            "Total Timesteps: 140083 Episode Num: 162 Reward: 447.21926522290386\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 508.763201\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 141083 Episode Num: 163 Reward: 500.2382200888707\n",
            "Total Timesteps: 142083 Episode Num: 164 Reward: 523.4700686251032\n",
            "Total Timesteps: 143083 Episode Num: 165 Reward: 527.9509260557692\n",
            "Total Timesteps: 144083 Episode Num: 166 Reward: 575.3379436042947\n",
            "Total Timesteps: 145083 Episode Num: 167 Reward: 616.0496422797322\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 515.353814\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 146083 Episode Num: 168 Reward: 586.117625687949\n",
            "Total Timesteps: 147083 Episode Num: 169 Reward: 272.3443805060706\n",
            "Total Timesteps: 148083 Episode Num: 170 Reward: 472.9958627608564\n",
            "Total Timesteps: 149083 Episode Num: 171 Reward: 231.6650443782338\n",
            "Total Timesteps: 149123 Episode Num: 172 Reward: 4.92400751562552\n",
            "Total Timesteps: 149169 Episode Num: 173 Reward: 9.036324711655991\n",
            "Total Timesteps: 149206 Episode Num: 174 Reward: 8.351430481236338\n",
            "Total Timesteps: 149249 Episode Num: 175 Reward: 8.649918372856623\n",
            "Total Timesteps: 149303 Episode Num: 176 Reward: 13.070439225800301\n",
            "Total Timesteps: 149404 Episode Num: 177 Reward: 36.568560428541495\n",
            "Total Timesteps: 149571 Episode Num: 178 Reward: 57.237133475536915\n",
            "Total Timesteps: 149813 Episode Num: 179 Reward: 62.86316475903519\n",
            "Total Timesteps: 150077 Episode Num: 180 Reward: 76.5462772644812\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 8.217468\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 150098 Episode Num: 181 Reward: 0.9824842658706585\n",
            "Total Timesteps: 150127 Episode Num: 182 Reward: 4.752122373954058\n",
            "Total Timesteps: 150200 Episode Num: 183 Reward: 22.41342521711018\n",
            "Total Timesteps: 150238 Episode Num: 184 Reward: 6.398826425831425\n",
            "Total Timesteps: 150259 Episode Num: 185 Reward: 0.8912285162792317\n",
            "Total Timesteps: 150401 Episode Num: 186 Reward: 34.96199104713888\n",
            "Total Timesteps: 150433 Episode Num: 187 Reward: 3.144702691400089\n",
            "Total Timesteps: 150453 Episode Num: 188 Reward: -0.1719465072261228\n",
            "Total Timesteps: 150473 Episode Num: 189 Reward: 0.15680698284336314\n",
            "Total Timesteps: 150494 Episode Num: 190 Reward: 1.5243750664678364\n",
            "Total Timesteps: 150514 Episode Num: 191 Reward: -1.1007932160026654\n",
            "Total Timesteps: 150534 Episode Num: 192 Reward: 0.7579213006948358\n",
            "Total Timesteps: 150562 Episode Num: 193 Reward: 6.070235346775413\n",
            "Total Timesteps: 150591 Episode Num: 194 Reward: 5.624203138553743\n",
            "Total Timesteps: 150726 Episode Num: 195 Reward: 54.60876276303854\n",
            "Total Timesteps: 150825 Episode Num: 196 Reward: 44.25515145031031\n",
            "Total Timesteps: 150868 Episode Num: 197 Reward: 7.493258465581411\n",
            "Total Timesteps: 150908 Episode Num: 198 Reward: -1.8996999881396688\n",
            "Total Timesteps: 150948 Episode Num: 199 Reward: 5.776222122440372\n",
            "Total Timesteps: 150986 Episode Num: 200 Reward: 4.801168797771587\n",
            "Total Timesteps: 151022 Episode Num: 201 Reward: 0.5197868713243028\n",
            "Total Timesteps: 151059 Episode Num: 202 Reward: 6.347193395762412\n",
            "Total Timesteps: 151093 Episode Num: 203 Reward: 3.373665764212902\n",
            "Total Timesteps: 151131 Episode Num: 204 Reward: 5.864236512307231\n",
            "Total Timesteps: 151168 Episode Num: 205 Reward: 2.009019532725945\n",
            "Total Timesteps: 151203 Episode Num: 206 Reward: 0.6974660154463499\n",
            "Total Timesteps: 151242 Episode Num: 207 Reward: 6.801908948088098\n",
            "Total Timesteps: 151282 Episode Num: 208 Reward: 6.792622861118747\n",
            "Total Timesteps: 151318 Episode Num: 209 Reward: 2.119511731334252\n",
            "Total Timesteps: 151367 Episode Num: 210 Reward: 14.233707474685572\n",
            "Total Timesteps: 151403 Episode Num: 211 Reward: 3.864039113734529\n",
            "Total Timesteps: 151440 Episode Num: 212 Reward: -2.407680358466468\n",
            "Total Timesteps: 151487 Episode Num: 213 Reward: 10.63030687045667\n",
            "Total Timesteps: 151543 Episode Num: 214 Reward: 9.139201157253204\n",
            "Total Timesteps: 152543 Episode Num: 215 Reward: 305.3578439795411\n",
            "Total Timesteps: 153543 Episode Num: 216 Reward: 453.3685291235481\n",
            "Total Timesteps: 154543 Episode Num: 217 Reward: 418.2783043817667\n",
            "Total Timesteps: 155543 Episode Num: 218 Reward: 389.5983588144023\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 190.601590\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 156543 Episode Num: 219 Reward: 190.91253540361438\n",
            "Total Timesteps: 157543 Episode Num: 220 Reward: 288.5773407689107\n",
            "Total Timesteps: 158543 Episode Num: 221 Reward: 484.3385511759589\n",
            "Total Timesteps: 159543 Episode Num: 222 Reward: 297.4961493947087\n",
            "Total Timesteps: 160175 Episode Num: 223 Reward: 137.14873179881025\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 269.968146\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 161175 Episode Num: 224 Reward: 221.55332923168277\n",
            "Total Timesteps: 162175 Episode Num: 225 Reward: 306.5470189665518\n",
            "Total Timesteps: 163175 Episode Num: 226 Reward: 490.0756608093488\n",
            "Total Timesteps: 164175 Episode Num: 227 Reward: 479.1685259682057\n",
            "Total Timesteps: 165175 Episode Num: 228 Reward: 441.9285235089214\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 509.380800\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 166175 Episode Num: 229 Reward: 477.8241374857884\n",
            "Total Timesteps: 167175 Episode Num: 230 Reward: 291.3917309689995\n",
            "Total Timesteps: 168175 Episode Num: 231 Reward: 330.224083600678\n",
            "Total Timesteps: 169175 Episode Num: 232 Reward: 459.669927973004\n",
            "Total Timesteps: 170175 Episode Num: 233 Reward: 749.8317484069983\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 516.104355\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 171175 Episode Num: 234 Reward: 417.7396206333509\n",
            "Total Timesteps: 172175 Episode Num: 235 Reward: 550.2463499697361\n",
            "Total Timesteps: 173175 Episode Num: 236 Reward: 648.3665878155585\n",
            "Total Timesteps: 174175 Episode Num: 237 Reward: 611.6303821443042\n",
            "Total Timesteps: 175175 Episode Num: 238 Reward: 408.8334618443747\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 732.598370\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 176175 Episode Num: 239 Reward: 630.8221449533436\n",
            "Total Timesteps: 177175 Episode Num: 240 Reward: 504.092282723423\n",
            "Total Timesteps: 178175 Episode Num: 241 Reward: 555.834196112019\n",
            "Total Timesteps: 179175 Episode Num: 242 Reward: 646.5053283953778\n",
            "Total Timesteps: 179301 Episode Num: 243 Reward: -21.5170796311872\n",
            "Total Timesteps: 180301 Episode Num: 244 Reward: 613.7633110731116\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 446.379173\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 181301 Episode Num: 245 Reward: 531.732563910259\n",
            "Total Timesteps: 182301 Episode Num: 246 Reward: 723.059679912318\n",
            "Total Timesteps: 183301 Episode Num: 247 Reward: 845.999195221688\n",
            "Total Timesteps: 184301 Episode Num: 248 Reward: 391.5394844256988\n",
            "Total Timesteps: 185301 Episode Num: 249 Reward: 690.3932055830097\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 650.028228\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 186301 Episode Num: 250 Reward: 578.0912632043625\n",
            "Total Timesteps: 187301 Episode Num: 251 Reward: 674.698823481665\n",
            "Total Timesteps: 188301 Episode Num: 252 Reward: 528.5499632564887\n",
            "Total Timesteps: 189301 Episode Num: 253 Reward: 578.8650897154557\n",
            "Total Timesteps: 190301 Episode Num: 254 Reward: 633.6996285342062\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 570.505540\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 191301 Episode Num: 255 Reward: 363.03554999371994\n",
            "Total Timesteps: 192301 Episode Num: 256 Reward: 631.2459058386804\n",
            "Total Timesteps: 193301 Episode Num: 257 Reward: 589.6112907134625\n",
            "Total Timesteps: 194301 Episode Num: 258 Reward: 448.4673957890981\n",
            "Total Timesteps: 195301 Episode Num: 259 Reward: 666.7656316956904\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 499.003464\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 196301 Episode Num: 260 Reward: 528.0234891107862\n",
            "Total Timesteps: 197301 Episode Num: 261 Reward: 572.6840763524713\n",
            "Total Timesteps: 198301 Episode Num: 262 Reward: 544.8398845735327\n",
            "Total Timesteps: 199301 Episode Num: 263 Reward: 693.4059382717776\n",
            "Total Timesteps: 200301 Episode Num: 264 Reward: 724.8676809326408\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 548.090778\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 201301 Episode Num: 265 Reward: 517.7696435166768\n",
            "Total Timesteps: 202301 Episode Num: 266 Reward: 848.8584039522791\n",
            "Total Timesteps: 203301 Episode Num: 267 Reward: 618.1051435977661\n",
            "Total Timesteps: 204301 Episode Num: 268 Reward: 445.6924583440902\n",
            "Total Timesteps: 205301 Episode Num: 269 Reward: 506.5147879643781\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 465.573849\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 206301 Episode Num: 270 Reward: 517.9583522483596\n",
            "Total Timesteps: 207301 Episode Num: 271 Reward: 628.9230718110442\n",
            "Total Timesteps: 208301 Episode Num: 272 Reward: 318.3739073382723\n",
            "Total Timesteps: 209301 Episode Num: 273 Reward: 152.86291922292816\n",
            "Total Timesteps: 210301 Episode Num: 274 Reward: 320.5639352753089\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 209.799192\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 211301 Episode Num: 275 Reward: 220.52886778552187\n",
            "Total Timesteps: 211835 Episode Num: 276 Reward: 77.06232695406784\n",
            "Total Timesteps: 212835 Episode Num: 277 Reward: 199.88187135208406\n",
            "Total Timesteps: 213835 Episode Num: 278 Reward: 238.36283604969094\n",
            "Total Timesteps: 213910 Episode Num: 279 Reward: 43.25452245453023\n",
            "Total Timesteps: 214910 Episode Num: 280 Reward: 321.61356759551245\n",
            "Total Timesteps: 215910 Episode Num: 281 Reward: 147.4513180251088\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 214.758627\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 216910 Episode Num: 282 Reward: 252.16001584500404\n",
            "Total Timesteps: 217910 Episode Num: 283 Reward: 274.39288386071996\n",
            "Total Timesteps: 218910 Episode Num: 284 Reward: 339.85975120992595\n",
            "Total Timesteps: 219910 Episode Num: 285 Reward: 551.5097436865932\n",
            "Total Timesteps: 220910 Episode Num: 286 Reward: 515.135286296686\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 616.000624\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 221910 Episode Num: 287 Reward: 731.3550180311377\n",
            "Total Timesteps: 222910 Episode Num: 288 Reward: 332.24130185207764\n",
            "Total Timesteps: 223910 Episode Num: 289 Reward: 312.28791632629043\n",
            "Total Timesteps: 224910 Episode Num: 290 Reward: 457.1109181748741\n",
            "Total Timesteps: 225910 Episode Num: 291 Reward: 249.84395169692317\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 262.128518\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 226910 Episode Num: 292 Reward: 340.1637896046473\n",
            "Total Timesteps: 227910 Episode Num: 293 Reward: 126.91500707189647\n",
            "Total Timesteps: 228910 Episode Num: 294 Reward: 575.4675672360651\n",
            "Total Timesteps: 229910 Episode Num: 295 Reward: 678.0913237031748\n",
            "Total Timesteps: 230910 Episode Num: 296 Reward: 678.2882649500958\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 623.356340\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 231910 Episode Num: 297 Reward: 720.1132652432051\n",
            "Total Timesteps: 232910 Episode Num: 298 Reward: 613.7589680725381\n",
            "Total Timesteps: 233910 Episode Num: 299 Reward: 814.4634510851804\n",
            "Total Timesteps: 234910 Episode Num: 300 Reward: 571.0663259703404\n",
            "Total Timesteps: 235910 Episode Num: 301 Reward: 762.7914398863899\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 635.968492\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 236910 Episode Num: 302 Reward: 748.2539158373482\n",
            "Total Timesteps: 237910 Episode Num: 303 Reward: 334.92956503622236\n",
            "Total Timesteps: 238910 Episode Num: 304 Reward: 517.8041860225841\n",
            "Total Timesteps: 239910 Episode Num: 305 Reward: 762.3468369316529\n",
            "Total Timesteps: 240910 Episode Num: 306 Reward: 550.5018216529938\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 692.501126\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 241910 Episode Num: 307 Reward: 647.7791365600689\n",
            "Total Timesteps: 242910 Episode Num: 308 Reward: 667.120190380617\n",
            "Total Timesteps: 243910 Episode Num: 309 Reward: 727.7133836812746\n",
            "Total Timesteps: 244910 Episode Num: 310 Reward: 708.0418045007314\n",
            "Total Timesteps: 245910 Episode Num: 311 Reward: 564.5635887984788\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 641.823661\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 246910 Episode Num: 312 Reward: 540.1017884510751\n",
            "Total Timesteps: 247910 Episode Num: 313 Reward: 571.9355591769705\n",
            "Total Timesteps: 248910 Episode Num: 314 Reward: 624.8211006259012\n",
            "Total Timesteps: 249910 Episode Num: 315 Reward: 617.8654094543717\n",
            "Total Timesteps: 250910 Episode Num: 316 Reward: 686.7618649106192\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 725.115395\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 251910 Episode Num: 317 Reward: 639.3446976767599\n",
            "Total Timesteps: 252910 Episode Num: 318 Reward: 758.3814987521528\n",
            "Total Timesteps: 253910 Episode Num: 319 Reward: 825.9927631235956\n",
            "Total Timesteps: 254910 Episode Num: 320 Reward: 833.5992726232981\n",
            "Total Timesteps: 255910 Episode Num: 321 Reward: 622.7815360021218\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 600.462525\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 256910 Episode Num: 322 Reward: 792.7132882245886\n",
            "Total Timesteps: 257910 Episode Num: 323 Reward: 643.8428991998187\n",
            "Total Timesteps: 258910 Episode Num: 324 Reward: 714.640682596898\n",
            "Total Timesteps: 259910 Episode Num: 325 Reward: 722.2222798495558\n",
            "Total Timesteps: 260910 Episode Num: 326 Reward: 697.7587674791106\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 582.378610\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 261910 Episode Num: 327 Reward: 528.6361182431307\n",
            "Total Timesteps: 262910 Episode Num: 328 Reward: 741.3766362592219\n",
            "Total Timesteps: 263910 Episode Num: 329 Reward: 778.8513152433308\n",
            "Total Timesteps: 264910 Episode Num: 330 Reward: 785.7549132382374\n",
            "Total Timesteps: 265910 Episode Num: 331 Reward: 731.4638207570235\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 734.840635\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 266910 Episode Num: 332 Reward: 740.4478049077298\n",
            "Total Timesteps: 267910 Episode Num: 333 Reward: 621.6023831276755\n",
            "Total Timesteps: 268910 Episode Num: 334 Reward: 593.9789140148242\n",
            "Total Timesteps: 269910 Episode Num: 335 Reward: 523.2628684985359\n",
            "Total Timesteps: 270910 Episode Num: 336 Reward: 792.8214509600414\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 800.166926\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 271910 Episode Num: 337 Reward: 826.5327023651006\n",
            "Total Timesteps: 272910 Episode Num: 338 Reward: 472.2716071388664\n",
            "Total Timesteps: 273910 Episode Num: 339 Reward: 763.4231828820489\n",
            "Total Timesteps: 274910 Episode Num: 340 Reward: 698.2810591796033\n",
            "Total Timesteps: 275910 Episode Num: 341 Reward: 879.243491880383\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 840.823596\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 276910 Episode Num: 342 Reward: 904.285302820657\n",
            "Total Timesteps: 277910 Episode Num: 343 Reward: 845.3129387665095\n",
            "Total Timesteps: 278910 Episode Num: 344 Reward: 500.38683487721073\n",
            "Total Timesteps: 279910 Episode Num: 345 Reward: 839.813670295969\n",
            "Total Timesteps: 280910 Episode Num: 346 Reward: 669.9935580977619\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 736.105413\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 281910 Episode Num: 347 Reward: 840.3132080944149\n",
            "Total Timesteps: 282910 Episode Num: 348 Reward: 808.5087358818014\n",
            "Total Timesteps: 283910 Episode Num: 349 Reward: 885.8257335748665\n",
            "Total Timesteps: 284910 Episode Num: 350 Reward: 626.7580183281627\n",
            "Total Timesteps: 285910 Episode Num: 351 Reward: 817.7597859707885\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 588.720587\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 286910 Episode Num: 352 Reward: 734.6870312096746\n",
            "Total Timesteps: 287910 Episode Num: 353 Reward: 963.0568421351129\n",
            "Total Timesteps: 288910 Episode Num: 354 Reward: 876.8402211177472\n",
            "Total Timesteps: 289910 Episode Num: 355 Reward: 731.1812463749661\n",
            "Total Timesteps: 290910 Episode Num: 356 Reward: 417.404290343817\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 709.769769\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 291910 Episode Num: 357 Reward: 672.1092213978692\n",
            "Total Timesteps: 292910 Episode Num: 358 Reward: 539.7488124328403\n",
            "Total Timesteps: 293910 Episode Num: 359 Reward: 702.3951113983562\n",
            "Total Timesteps: 294910 Episode Num: 360 Reward: 753.4447176224477\n",
            "Total Timesteps: 295910 Episode Num: 361 Reward: 802.5151704781823\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 832.439602\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 296910 Episode Num: 362 Reward: 713.7983900493683\n",
            "Total Timesteps: 297910 Episode Num: 363 Reward: 838.2564544396757\n",
            "Total Timesteps: 298910 Episode Num: 364 Reward: 905.1574815658208\n",
            "Total Timesteps: 299910 Episode Num: 365 Reward: 789.3954503006825\n",
            "Total Timesteps: 300910 Episode Num: 366 Reward: 662.8536652770326\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 769.254906\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 301910 Episode Num: 367 Reward: 828.2890019863645\n",
            "Total Timesteps: 302910 Episode Num: 368 Reward: 687.757102359904\n",
            "Total Timesteps: 303910 Episode Num: 369 Reward: 851.2049040237486\n",
            "Total Timesteps: 304910 Episode Num: 370 Reward: 748.0200663119458\n",
            "Total Timesteps: 305910 Episode Num: 371 Reward: 868.7349564838502\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 797.013916\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 306910 Episode Num: 372 Reward: 837.4371528156305\n",
            "Total Timesteps: 307910 Episode Num: 373 Reward: 975.1125897704094\n",
            "Total Timesteps: 308910 Episode Num: 374 Reward: 800.0608243967197\n",
            "Total Timesteps: 309910 Episode Num: 375 Reward: 975.8309373012715\n",
            "Total Timesteps: 310910 Episode Num: 376 Reward: 646.698894477906\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 716.814812\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 311910 Episode Num: 377 Reward: 927.9326499778964\n",
            "Total Timesteps: 312910 Episode Num: 378 Reward: 967.9386683053632\n",
            "Total Timesteps: 313910 Episode Num: 379 Reward: 971.0695460902082\n",
            "Total Timesteps: 314910 Episode Num: 380 Reward: 878.0132735663284\n",
            "Total Timesteps: 315910 Episode Num: 381 Reward: 933.3949540642941\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 913.241998\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 316910 Episode Num: 382 Reward: 431.24271314972117\n",
            "Total Timesteps: 317910 Episode Num: 383 Reward: 527.4277074800416\n",
            "Total Timesteps: 318910 Episode Num: 384 Reward: 583.1740617389819\n",
            "Total Timesteps: 319910 Episode Num: 385 Reward: 434.1218684066727\n",
            "Total Timesteps: 320910 Episode Num: 386 Reward: 922.3780909596472\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 868.073848\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 321910 Episode Num: 387 Reward: 975.2427646009273\n",
            "Total Timesteps: 322910 Episode Num: 388 Reward: 940.0115871051505\n",
            "Total Timesteps: 323910 Episode Num: 389 Reward: 968.808137159968\n",
            "Total Timesteps: 324910 Episode Num: 390 Reward: 546.789376016429\n",
            "Total Timesteps: 325910 Episode Num: 391 Reward: 745.2773194215852\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 790.604322\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 326910 Episode Num: 392 Reward: 367.79539461164757\n",
            "Total Timesteps: 327910 Episode Num: 393 Reward: 916.8380310193012\n",
            "Total Timesteps: 328910 Episode Num: 394 Reward: 892.0520291798928\n",
            "Total Timesteps: 329910 Episode Num: 395 Reward: 963.9279747088952\n",
            "Total Timesteps: 330910 Episode Num: 396 Reward: 920.1445762820609\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 898.310616\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 331910 Episode Num: 397 Reward: 477.70674660785266\n",
            "Total Timesteps: 332910 Episode Num: 398 Reward: 883.9888522342937\n",
            "Total Timesteps: 333910 Episode Num: 399 Reward: 935.3829790932102\n",
            "Total Timesteps: 334910 Episode Num: 400 Reward: 972.1323110262733\n",
            "Total Timesteps: 335910 Episode Num: 401 Reward: 716.5703465122418\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 898.017779\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 336910 Episode Num: 402 Reward: 918.0916772523774\n",
            "Total Timesteps: 337910 Episode Num: 403 Reward: 1064.4207591638053\n",
            "Total Timesteps: 338910 Episode Num: 404 Reward: 850.4401481142869\n",
            "Total Timesteps: 339910 Episode Num: 405 Reward: 899.8801491531384\n",
            "Total Timesteps: 340910 Episode Num: 406 Reward: 956.9776857035491\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 780.557331\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 341910 Episode Num: 407 Reward: 913.1982119511813\n",
            "Total Timesteps: 342910 Episode Num: 408 Reward: 955.3064293513827\n",
            "Total Timesteps: 343910 Episode Num: 409 Reward: 751.9528079576644\n",
            "Total Timesteps: 344910 Episode Num: 410 Reward: 693.3307021348592\n",
            "Total Timesteps: 345910 Episode Num: 411 Reward: 918.9366271194477\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 723.918363\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 346910 Episode Num: 412 Reward: 570.4199841887298\n",
            "Total Timesteps: 347910 Episode Num: 413 Reward: 927.8602101062032\n",
            "Total Timesteps: 348910 Episode Num: 414 Reward: 849.599641798116\n",
            "Total Timesteps: 349910 Episode Num: 415 Reward: 964.3473333332892\n",
            "Total Timesteps: 350910 Episode Num: 416 Reward: 992.8596064552244\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 827.013626\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 351910 Episode Num: 417 Reward: 782.1432518719706\n",
            "Total Timesteps: 352910 Episode Num: 418 Reward: 936.2319068878552\n",
            "Total Timesteps: 353910 Episode Num: 419 Reward: 993.1093622721423\n",
            "Total Timesteps: 354910 Episode Num: 420 Reward: 1010.4288564362492\n",
            "Total Timesteps: 355910 Episode Num: 421 Reward: 940.4670774414083\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 710.542615\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 356910 Episode Num: 422 Reward: 999.1906362803174\n",
            "Total Timesteps: 357910 Episode Num: 423 Reward: 1039.21415234889\n",
            "Total Timesteps: 358910 Episode Num: 424 Reward: 461.9510562872417\n",
            "Total Timesteps: 359910 Episode Num: 425 Reward: 882.1069136077199\n",
            "Total Timesteps: 360910 Episode Num: 426 Reward: 1169.3677937070006\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 945.306169\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 361910 Episode Num: 427 Reward: 1134.5662556776963\n",
            "Total Timesteps: 362910 Episode Num: 428 Reward: 907.2583212449478\n",
            "Total Timesteps: 363910 Episode Num: 429 Reward: 484.43774963509657\n",
            "Total Timesteps: 364910 Episode Num: 430 Reward: 1171.613966044942\n",
            "Total Timesteps: 365910 Episode Num: 431 Reward: 1049.818351081231\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1045.303741\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 366910 Episode Num: 432 Reward: 1141.9571972555816\n",
            "Total Timesteps: 367910 Episode Num: 433 Reward: 1237.8115907054803\n",
            "Total Timesteps: 368910 Episode Num: 434 Reward: 1137.1087947153383\n",
            "Total Timesteps: 369910 Episode Num: 435 Reward: 1138.6284691167773\n",
            "Total Timesteps: 370910 Episode Num: 436 Reward: 907.3767486927184\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1020.658253\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 371358 Episode Num: 437 Reward: 438.2534379897376\n",
            "Total Timesteps: 372358 Episode Num: 438 Reward: 922.0803978204357\n",
            "Total Timesteps: 373358 Episode Num: 439 Reward: 883.061697470005\n",
            "Total Timesteps: 374358 Episode Num: 440 Reward: 1183.7393077954011\n",
            "Total Timesteps: 375358 Episode Num: 441 Reward: 951.109566019566\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1025.701934\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 376358 Episode Num: 442 Reward: 1051.4848438277438\n",
            "Total Timesteps: 377358 Episode Num: 443 Reward: 1163.8069232606395\n",
            "Total Timesteps: 378358 Episode Num: 444 Reward: 1019.8085228165464\n",
            "Total Timesteps: 379358 Episode Num: 445 Reward: 959.869729909743\n",
            "Total Timesteps: 380358 Episode Num: 446 Reward: 772.9595607165556\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1287.093016\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 381358 Episode Num: 447 Reward: 1382.9047211209224\n",
            "Total Timesteps: 382358 Episode Num: 448 Reward: 1477.8067438579826\n",
            "Total Timesteps: 383358 Episode Num: 449 Reward: 1226.6107709197659\n",
            "Total Timesteps: 384358 Episode Num: 450 Reward: 1392.657673207407\n",
            "Total Timesteps: 385358 Episode Num: 451 Reward: 1384.793363990627\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1046.187809\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 386358 Episode Num: 452 Reward: 892.649253770701\n",
            "Total Timesteps: 387358 Episode Num: 453 Reward: 1299.2084036629292\n",
            "Total Timesteps: 388358 Episode Num: 454 Reward: 1035.1988778805946\n",
            "Total Timesteps: 389358 Episode Num: 455 Reward: 1428.6367274619497\n",
            "Total Timesteps: 390358 Episode Num: 456 Reward: 1029.9449120409276\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1139.063322\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 391358 Episode Num: 457 Reward: 1270.4667397313035\n",
            "Total Timesteps: 392358 Episode Num: 458 Reward: 1092.2615804933002\n",
            "Total Timesteps: 393358 Episode Num: 459 Reward: 1268.5143227731467\n",
            "Total Timesteps: 394358 Episode Num: 460 Reward: 1463.6701440418306\n",
            "Total Timesteps: 395358 Episode Num: 461 Reward: 1394.8050550920557\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1333.046218\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 396358 Episode Num: 462 Reward: 1450.6961029182805\n",
            "Total Timesteps: 397358 Episode Num: 463 Reward: 1615.1295534982364\n",
            "Total Timesteps: 398358 Episode Num: 464 Reward: 1562.050785998503\n",
            "Total Timesteps: 399358 Episode Num: 465 Reward: 1533.87692521895\n",
            "Total Timesteps: 400358 Episode Num: 466 Reward: 1366.994781412665\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1501.164595\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 401358 Episode Num: 467 Reward: 1540.6920263916686\n",
            "Total Timesteps: 402358 Episode Num: 468 Reward: 1618.9031470576126\n",
            "Total Timesteps: 403358 Episode Num: 469 Reward: 1486.7621117423325\n",
            "Total Timesteps: 404358 Episode Num: 470 Reward: 1685.9021255436685\n",
            "Total Timesteps: 405358 Episode Num: 471 Reward: 1464.3246999381265\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1633.784065\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 406358 Episode Num: 472 Reward: 1618.399851136705\n",
            "Total Timesteps: 407358 Episode Num: 473 Reward: 1572.7977825310418\n",
            "Total Timesteps: 408358 Episode Num: 474 Reward: 1514.0307960325292\n",
            "Total Timesteps: 409358 Episode Num: 475 Reward: 1685.8794034668988\n",
            "Total Timesteps: 410358 Episode Num: 476 Reward: 1684.9110447408107\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1452.875743\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 411358 Episode Num: 477 Reward: 1381.979648129327\n",
            "Total Timesteps: 412358 Episode Num: 478 Reward: 1581.7294246061874\n",
            "Total Timesteps: 413358 Episode Num: 479 Reward: 1760.1567125896897\n",
            "Total Timesteps: 414358 Episode Num: 480 Reward: 1638.9618844930458\n",
            "Total Timesteps: 415358 Episode Num: 481 Reward: 1458.8808815479729\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1605.218251\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 416358 Episode Num: 482 Reward: 1616.297627442091\n",
            "Total Timesteps: 417358 Episode Num: 483 Reward: 1502.2804758823818\n",
            "Total Timesteps: 418358 Episode Num: 484 Reward: 1447.691606749306\n",
            "Total Timesteps: 419358 Episode Num: 485 Reward: 1696.7058269894076\n",
            "Total Timesteps: 420358 Episode Num: 486 Reward: 1706.8252404578623\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1704.610385\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 421358 Episode Num: 487 Reward: 1695.0163468749458\n",
            "Total Timesteps: 422358 Episode Num: 488 Reward: 1509.8600707279838\n",
            "Total Timesteps: 423358 Episode Num: 489 Reward: 1554.7026350896983\n",
            "Total Timesteps: 424358 Episode Num: 490 Reward: 1628.309587195904\n",
            "Total Timesteps: 425358 Episode Num: 491 Reward: 1574.7246307384387\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1559.691070\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 426358 Episode Num: 492 Reward: 1539.109108001599\n",
            "Total Timesteps: 427358 Episode Num: 493 Reward: 1792.951596883331\n",
            "Total Timesteps: 428358 Episode Num: 494 Reward: 1718.0480775156614\n",
            "Total Timesteps: 429358 Episode Num: 495 Reward: 1813.3363829983293\n",
            "Total Timesteps: 430358 Episode Num: 496 Reward: 1476.338666886712\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1789.120509\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 431358 Episode Num: 497 Reward: 1798.9242003811396\n",
            "Total Timesteps: 432358 Episode Num: 498 Reward: 1587.0834820298505\n",
            "Total Timesteps: 433358 Episode Num: 499 Reward: 1739.8521467134287\n",
            "Total Timesteps: 434358 Episode Num: 500 Reward: 1665.3123368022984\n",
            "Total Timesteps: 435358 Episode Num: 501 Reward: 1784.252147946048\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1844.725939\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 436358 Episode Num: 502 Reward: 1830.9217457872478\n",
            "Total Timesteps: 437358 Episode Num: 503 Reward: 1604.1311179546697\n",
            "Total Timesteps: 438358 Episode Num: 504 Reward: 1845.3247113716197\n",
            "Total Timesteps: 439358 Episode Num: 505 Reward: 1685.9183698576428\n",
            "Total Timesteps: 440358 Episode Num: 506 Reward: 1606.331278502465\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1511.090993\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 441358 Episode Num: 507 Reward: 1487.8309870640485\n",
            "Total Timesteps: 442358 Episode Num: 508 Reward: 1562.260876185669\n",
            "Total Timesteps: 443358 Episode Num: 509 Reward: 1731.2025965089535\n",
            "Total Timesteps: 444358 Episode Num: 510 Reward: 1648.3525129388174\n",
            "Total Timesteps: 445358 Episode Num: 511 Reward: 1648.8937598799932\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1662.194806\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 446358 Episode Num: 512 Reward: 1664.8259587180194\n",
            "Total Timesteps: 447358 Episode Num: 513 Reward: 1731.2150852889538\n",
            "Total Timesteps: 448358 Episode Num: 514 Reward: 1823.410520005251\n",
            "Total Timesteps: 449358 Episode Num: 515 Reward: 1738.0908883327147\n",
            "Total Timesteps: 450358 Episode Num: 516 Reward: 1761.404303320728\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1706.287589\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 451358 Episode Num: 517 Reward: 1687.8083908244032\n",
            "Total Timesteps: 452358 Episode Num: 518 Reward: 1767.0756293381712\n",
            "Total Timesteps: 453358 Episode Num: 519 Reward: 1711.1487369816502\n",
            "Total Timesteps: 454358 Episode Num: 520 Reward: 1874.5983639414333\n",
            "Total Timesteps: 455358 Episode Num: 521 Reward: 1892.269421999447\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1707.031993\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 456358 Episode Num: 522 Reward: 1702.651368045663\n",
            "Total Timesteps: 457358 Episode Num: 523 Reward: 1847.5350616577962\n",
            "Total Timesteps: 458358 Episode Num: 524 Reward: 1778.2522869555546\n",
            "Total Timesteps: 459358 Episode Num: 525 Reward: 1806.9839565302625\n",
            "Total Timesteps: 460358 Episode Num: 526 Reward: 1743.0195039669907\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1821.703542\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 461358 Episode Num: 527 Reward: 1805.1854129689361\n",
            "Total Timesteps: 462358 Episode Num: 528 Reward: 1831.9545837883684\n",
            "Total Timesteps: 463358 Episode Num: 529 Reward: 1761.1407677363766\n",
            "Total Timesteps: 464358 Episode Num: 530 Reward: 1748.1522778807043\n",
            "Total Timesteps: 465358 Episode Num: 531 Reward: 1822.422993158296\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1867.459162\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 466358 Episode Num: 532 Reward: 1854.6665280411373\n",
            "Total Timesteps: 467358 Episode Num: 533 Reward: 1803.3068741651807\n",
            "Total Timesteps: 468358 Episode Num: 534 Reward: 1798.7690470057173\n",
            "Total Timesteps: 469358 Episode Num: 535 Reward: 1726.417065454852\n",
            "Total Timesteps: 470358 Episode Num: 536 Reward: 1712.6590488007928\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1777.155597\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 471358 Episode Num: 537 Reward: 1757.6433772338164\n",
            "Total Timesteps: 472358 Episode Num: 538 Reward: 1802.2955561503156\n",
            "Total Timesteps: 473358 Episode Num: 539 Reward: 1652.3135608128923\n",
            "Total Timesteps: 474358 Episode Num: 540 Reward: 1648.9377883345235\n",
            "Total Timesteps: 475358 Episode Num: 541 Reward: 1735.8286267674455\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1905.901421\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 476358 Episode Num: 542 Reward: 1885.4046364913709\n",
            "Total Timesteps: 477358 Episode Num: 543 Reward: 1867.589444558466\n",
            "Total Timesteps: 478358 Episode Num: 544 Reward: 1892.4143256524792\n",
            "Total Timesteps: 479358 Episode Num: 545 Reward: 1840.1803940185184\n",
            "Total Timesteps: 480358 Episode Num: 546 Reward: 1626.842991858377\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1800.223402\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 481358 Episode Num: 547 Reward: 1797.1764245261975\n",
            "Total Timesteps: 482358 Episode Num: 548 Reward: 1617.6157406420205\n",
            "Total Timesteps: 483358 Episode Num: 549 Reward: 1826.472971655425\n",
            "Total Timesteps: 484358 Episode Num: 550 Reward: 1943.6900795923618\n",
            "Total Timesteps: 485358 Episode Num: 551 Reward: 1884.590541801815\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1842.977515\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 486358 Episode Num: 552 Reward: 1826.5429310828265\n",
            "Total Timesteps: 487358 Episode Num: 553 Reward: 1902.8891105884993\n",
            "Total Timesteps: 488358 Episode Num: 554 Reward: 1827.325484588359\n",
            "Total Timesteps: 489358 Episode Num: 555 Reward: 1848.6933083236706\n",
            "Total Timesteps: 490358 Episode Num: 556 Reward: 1968.758370659425\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1882.140160\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 491358 Episode Num: 557 Reward: 1877.4637009195421\n",
            "Total Timesteps: 492358 Episode Num: 558 Reward: 1880.1218998620734\n",
            "Total Timesteps: 493358 Episode Num: 559 Reward: 1806.8183958660288\n",
            "Total Timesteps: 494358 Episode Num: 560 Reward: 1805.1813466958665\n",
            "Total Timesteps: 495358 Episode Num: 561 Reward: 1740.5827231071255\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1784.170872\n",
            "-------------------------------------------------\n",
            "Total Timesteps: 496358 Episode Num: 562 Reward: 1783.5102497083576\n",
            "Total Timesteps: 497358 Episode Num: 563 Reward: 1796.2821214182616\n",
            "Total Timesteps: 498358 Episode Num: 564 Reward: 1934.0834860424727\n",
            "Total Timesteps: 499358 Episode Num: 565 Reward: 1624.9680833232828\n",
            "-------------------------------------------------\n",
            "Recompensa promedio en el paso de Evaluación: 1845.942759\n",
            "-------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e"
      },
      "source": [
        "## Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Definimos el primero de los Críticos como red neuronal profunda\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Definimos el segundo de los Críticos como red neuronal profunda\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Propagación hacia adelante del primero de los Críticos\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Propagación hacia adelante del segundo de los Críticos\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "  \n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selección del dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construir todo el proceso de entrenamiento en una clase\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clipping=0.5, policy_freq=2):\n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Paso 4: Tomamos una muestra de transiciones (s, s’, a, r) de la memoria.\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Paso 5: A partir del estado siguiente s', el Actor del Target ejecuta la siguiente acción a'.\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Paso 6: Añadimos ruido gaussiano a la siguiente acción a' y lo cortamos para tenerlo en el rango de valores aceptado por el entorno.\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device) \n",
        "      noise = noise.clamp(-noise_clipping, noise_clipping)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Paso 7: Los dos Críticos del Target toman un par (s’, a’) como entrada y devuelven dos Q-values Qt1(s’,a’) y Qt2(s’,a’) como salida.\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Paso 8: Nos quedamos con el mínimo de los dos Q-values: min(Qt1, Qt2). Representa el valor aproximado del estado siguiente.\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Paso 9: Obtenemos el target final de los dos Crítico del Modelo, que es: Qt = r + γ * min(Qt1, Qt2), donde γ es el factor de descuento.\n",
        "      target_Q = reward + ((1-done) * discount * target_Q).detach()\n",
        "\n",
        "      # Paso 10: Los dos Críticos del Modelo toman un par (s, a) como entrada y devuelven dos Q-values Q1(s,a) y Q2(s,a) como salida.\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Paso 11: Calculamos la pérdida procedente de los Crítico del Modelo: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Paso 12: Propagamos hacia atrás la pérdida del crítico y actualizamos los parámetros de los dos Crítico del Modelo con un SGD.\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Paso 13: Cada dos iteraciones, actualizamos nuestro modelo de Actor ejecutando el gradiente ascendente en la salida del primer modelo crítico.\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_optimizer.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Paso 14: Todavía cada dos iteraciones, actualizamos los pesos del Actor del Target usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "        # Paso 15: Todavía cada dos iteraciones, actualizamos los pesos del target del Crítico usando el promedio Polyak.\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "  # Método para guardar el modelo entrenado\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n",
        "\n",
        "  # Método para cargar el modelo entrenado\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load(\"%s/%s_actor.pth\" % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load(\"%s/%s_critic.pth\" % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"------------------------------------------------\")\n",
        "  print (\"Recompensa promedio en el paso de Evaluación: %f\" % (avg_reward))\n",
        "  print (\"------------------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"HalfCheetahBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Configuración: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}